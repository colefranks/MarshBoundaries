{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e60d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#cv2 a.k.a. opencv is a package for dealing with images\n",
    "import cv2 as cv\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math as mh\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd898161",
   "metadata": {},
   "source": [
    "# Getting and preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd26c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD IMAGES\n",
    "# chose this size so that it approximates images well but is also highly divisible by 2\n",
    "# because some of the neural network layers shrink the size by factor of 2\n",
    "\n",
    "XDIM = 300 #2816\n",
    "YDIM = 300 #11*XDIM//8 \n",
    "\n",
    "def load_marsh_images(folder):\n",
    "    images = []\n",
    "    \n",
    "    for i in range(1,12):\n",
    "        # reading PIE\n",
    "        #img = cv.imread(os.path.join(folder,\"LC08_L1TP_012030_20130404_20170310_01_T1_B%d_subset.png\" % i))\n",
    "        # reading VCR\n",
    "        img = cv.imread(os.path.join(folder,\"LC08_L1TP_014034_20130516_20170310_01_T1_B%d_subset.png\" % i))\n",
    "        # reading GCE\n",
    "        #img = cv.imread(os.path.join(folder,\"LC08_L1TP_016038_20130328_20170310_01_T1_B%d_subset.png\" % i))\n",
    "        if img is not None:\n",
    "            # resize our higher resolution band 8 to be the same with others\n",
    "            if (i == 8):\n",
    "                img = cv.resize(img,(xDIM, yDIM))\n",
    "            else:\n",
    "                yDIM = img.shape[0]\n",
    "                xDIM = img.shape[1]\n",
    "            #padding/resizing them with white so they are the same size\n",
    "            img = cv.copyMakeBorder(img,mh.ceil((YDIM-yDIM)/2),mh.floor((YDIM-yDIM)/2),mh.ceil((XDIM-xDIM)/2),mh.floor((XDIM-xDIM)/2),cv.BORDER_CONSTANT, None, value = 0)\n",
    "            #img = cv.resize(img,(XDIM, YDIM))\n",
    "            #convert to black and white\n",
    "            img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "            #subset images to zoom in\n",
    "            img = img #img[199:499,199:399] for VCR\n",
    "            #warp so the mask is square\n",
    "            #img = cv.warpPerspective(img,Mpad,(SIZE+2*PAD,SIZE+2*PAD))\n",
    "            images.append(img)\n",
    "    return images\n",
    "\n",
    "#get images with PIE thrown out\n",
    "\n",
    "#gce_imgs = load_marsh_images(\"train_jpg/GCE\")\n",
    "#pie_imgs = load_marsh_images(\"train_jpg/PIE\")\n",
    "#vcr_imgs = load_marsh_images(\"train_jpg/VCR\")\n",
    "\n",
    "#gce_imgs = load_marsh_images(\"train_jpg/Jul2023/GCE\")\n",
    "#pie_imgs = load_marsh_images(\"train_jpg/Jul2023/PIE\")\n",
    "vcr_imgs = load_marsh_images(\"train_jpg/Jul2023/VCR\")\n",
    "\n",
    "images = np.array([\n",
    "    #gce_imgs\n",
    "    #pie_imgs\n",
    "    vcr_imgs\n",
    "    ])\n",
    "NUM_SITE = images.shape[0]\n",
    "#YDIM=images.shape[2]\n",
    "#XDIM=images.shape[3]\n",
    "#XDIM=images[site_num,5].shape[0]\n",
    "#YDIM=images[site_num,5].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6e82d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view one of our beautiful images\n",
    "site_num=0 #selecting site to train\n",
    "plt.imshow(images[site_num,5])\n",
    "print(images[site_num,5].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30ef41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET MASKS\n",
    "\n",
    "\n",
    "#gce_mask = cv.imread(\"new-masks/updated_GCE_mask.jpg\")\n",
    "#pie_mask = cv.imread(\"new-masks/updated_PIE_mask.jpg\")\n",
    "#vcr_mask = cv.imread(\"new-masks/updated_VCR_map.jpg\")\n",
    "\n",
    "#new data\n",
    "\n",
    "#gce_mask = cv.imread(\"new-masks/Jul2023/ready for ML GCE1.png\")\n",
    "#gce_mask = cv.resize(gce_mask,(xDIM,yDIM))\n",
    "#pie_mask = cv.imread(\"new-masks/Jul2023/ready for ML PIE1.png\")\n",
    "#pie_mask = cv.resize(pie_mask,(xDIM,yDIM))\n",
    "vcr_mask = cv.imread(\"new-masks/Jul2023/ready for ML VCR1.png\")\n",
    "#print(vcr_mask.shape)\n",
    "#vcr_mask = cv.resize(vcr_mask,(xDIM,yDIM))\n",
    "#subset masks to zoom in\n",
    "#vcr_mask = vcr_mask[199:499,199:399]\n",
    "\n",
    "new_masks = [\n",
    "         #cv.cvtColor(gce_mask, cv.COLOR_BGR2GRAY) \n",
    "         #cv.cvtColor(pie_mask, cv.COLOR_BGR2GRAY) \n",
    "         cv.cvtColor(vcr_mask, cv.COLOR_BGR2GRAY)\n",
    "            ]\n",
    "# exploring data to set thresholds\n",
    "#plt.imshow(new_masks[0])\n",
    "#plt.colorbar()\n",
    "#plt.hist(new_masks[0])\n",
    "#plt.show\n",
    "\n",
    "#new_masks = [gce_mask,\n",
    "#            pie_mask,\n",
    "#            vcr_mask]\n",
    "\n",
    "masks = []\n",
    "for m in new_masks:\n",
    "    #m = cv.resize(m,(YDIM,XDIM))\n",
    "    # padding the masks in the same way with imgs so they match sizes\n",
    "    yDIM = m.shape[0]\n",
    "    xDIM = m.shape[1]    \n",
    "    m = cv.copyMakeBorder(m,mh.ceil((YDIM-yDIM)/2),mh.floor((YDIM-yDIM)/2),mh.ceil((XDIM-xDIM)/2),mh.floor((XDIM-xDIM)/2),cv.BORDER_CONSTANT, None, value = 0)\n",
    "    masks.append(m)\n",
    "new_masks = masks\n",
    "print(new_masks[0].shape)\n",
    "#YDIM = new_masks[0].shape[0]\n",
    "#XDIM = new_masks[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec4bef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPROCESS MASKS\n",
    "#change the values in the mask to integers from 1 to 5 instead of the weird \n",
    "# 0-255 values currently in there. Ideally the GIS software would output them like \n",
    "# this already, but c'est la vie.\n",
    "\n",
    "#hist, bins = np.histogram(new_masks[site_num], bins=85)\n",
    "\n",
    "#print (hist)\n",
    "#print (bins)\n",
    "\n",
    "THRESH = [10,26,40,60,76] #found by exploring the data\n",
    "# VCR        [10,          26,     40,      60,      76]\n",
    "#    unlabeled   tidal_flat  upland    pond    channel  marsh\n",
    "# GCE        [10,      20,         46,      60]\n",
    "#    unlabeled   marsh    tidal_flat  upland  channel\n",
    "# PIE        [12,      20,         46,      55]\n",
    "#    unlabeled   marsh    ponds      upland  channel \n",
    "new_mask = np.array(new_masks[0])\n",
    "unlabeled = (new_mask <= THRESH[0])\n",
    "tidal_flat = np.logical_and(THRESH[0] < new_mask, new_mask <= THRESH[1])\n",
    "upland = np.logical_and(THRESH[1] < new_mask, new_mask <= THRESH[2])\n",
    "pond = np.logical_and(THRESH[2] < new_mask, new_mask <= THRESH[3])\n",
    "channel = np.logical_and(THRESH[3] < new_mask, new_mask <= THRESH[4])\n",
    "marsh = THRESH[4] < new_mask\n",
    "\n",
    "#to throw away tidal_flat\n",
    "unlabeled = np.logical_or(tidal_flat,unlabeled)\n",
    "#to throw away pond\n",
    "unlabeled = np.logical_or(pond,unlabeled)\n",
    "masks = [channel,marsh,upland,unlabeled]\n",
    "\n",
    "int_mask = np.zeros_like(new_mask,dtype=int)\n",
    "for i,m in enumerate(masks):\n",
    "    int_mask = int_mask + i*m.astype(int)\n",
    "masks = np.reshape(int_mask, (NUM_SITE,1,YDIM,XDIM))\n",
    "\n",
    "#get frequencies of each class\n",
    "ind,counts = np.unique(masks,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2940598b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the above image to set the marsh colors:\n",
    "\n",
    "class_dict = {0: 'water', 1: 'marsh', 2: 'upland', 3: 'unlabeled'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a6843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view one of our beautiful masks\n",
    "cmap = plt.cm.get_cmap('viridis', 4)\n",
    "plt.imshow(masks[site_num,0,:,:],vmax=3,vmin=0,cmap=cmap)\n",
    "cbar = plt.colorbar(ticks=[3/8 + i*(3.0/4.0) for i in range(4)],orientation='horizontal')\n",
    "cbar.set_ticklabels([class_dict[i] for i in range(len(class_dict))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8d150c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('numpymasks',masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99947d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fdfefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST AND TRAIN SPLITS\n",
    "# we need to hide part of each image from the neural net while training \n",
    "# so that we can later use the hidden part for testing.\n",
    "# I do a naive thing where I just convert the upper 5/8 of the mask to the unknown class.\n",
    "# I also convert to tensor (data format for tensorflow)\n",
    "\n",
    "#Yiyang\n",
    "# It seems that tensor requires a rectangular data format where we simply don't have due to mismatch dimensions from 3 sites.\n",
    "# we could padd the images or use something like tensor torch?\n",
    "\n",
    "not_care = np.max(masks)\n",
    "\n",
    "te_masks = np.zeros_like(masks)\n",
    "tr_masks = np.zeros_like(masks)\n",
    "\n",
    "te_masks.fill(not_care)\n",
    "tr_masks.fill(not_care)\n",
    "\n",
    "te_masks[:,0,:3*XDIM//8,:]=masks[:,0,:3*XDIM//8,:]\n",
    "tr_masks[:,0,3*XDIM//8:,:]=masks[:,0,3*XDIM//8:,:]\n",
    "\n",
    "\n",
    "def get_tensor(images):\n",
    "    images_tensor = tf.convert_to_tensor(images,dtype =tf.float32)\n",
    "    SAMPLES,BANDS,HEIGHT,WIDTH = images_tensor.shape\n",
    "    image_tensor = tf.transpose(images_tensor,[0,2,3,1])\n",
    "    return image_tensor\n",
    "\n",
    "train_images = get_tensor(images)\n",
    "\n",
    "train_masks = get_tensor(tr_masks)\n",
    "\n",
    "test_images = get_tensor(images)\n",
    "\n",
    "test_masks = get_tensor(te_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178e5d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view one of our testing masks\n",
    "plt.imshow(te_masks[site_num,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aee8bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE TENSORFLOW DATASET AND AUGMENT DATA\n",
    "# datasets are a data format that makes training fast\n",
    "# each augmentation transformation\n",
    "#changes image and mask in exactly the same way\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images,train_masks))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images,test_masks))\n",
    "\n",
    "\n",
    "class Augment(tf.keras.layers.Layer):\n",
    "  def __init__(self, seed=42):\n",
    "    super().__init__()\n",
    "    # both use the same seed, so they'll make the same random changes.\n",
    "    self.augment_inputs = tf.keras.Sequential([\n",
    "      tf.keras.layers.RandomFlip(\"horizontal_and_vertical\",seed=seed),\n",
    "      tf.keras.layers.RandomRotation(1.0,seed=seed),\n",
    "      tf.keras.layers.RandomZoom(.5, .5,seed =seed)\n",
    "    ])\n",
    "    self.augment_labels = tf.keras.Sequential([\n",
    "      tf.keras.layers.RandomFlip(\"horizontal_and_vertical\",seed=seed),\n",
    "      tf.keras.layers.RandomRotation(1.0,seed=seed),\n",
    "      tf.keras.layers.RandomZoom(.5, .5,seed =seed)\n",
    "    ])\n",
    "    \n",
    "  def call(self, inputs, labels):\n",
    "    inputs = self.augment_inputs(inputs)\n",
    "    labels = self.augment_labels(labels)\n",
    "    return inputs, labels\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    train_batches = (\n",
    "    train_dataset\n",
    "    .cache()\n",
    "    .shuffle(NUM_SITE)\n",
    "    .batch(NUM_SITE)\n",
    "    #.repeat()\n",
    "    #.map(Augment()) ## not augmenting for linreg!!\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
    "\n",
    "test_batches = test_dataset.batch(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ace168",
   "metadata": {},
   "source": [
    "# Defining our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96021b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear regression model - no hidden layer\n",
    "\n",
    "def shallow_model(output_channels:int):\n",
    "    inputs = tf.keras.layers.Input(shape=[None, None, 11])\n",
    "    \n",
    "    #down\n",
    "    x = tf.keras.layers.Conv2D(filters=output_channels,kernel_size=1,strides=1)(inputs)\n",
    "    \n",
    "    #x = last(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584c6379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression on small neighborhood a la George\n",
    "\n",
    "def shallow_model(output_channels:int):\n",
    "    inputs = tf.keras.layers.Input(shape=[None, None, 11])\n",
    "    \n",
    "    #down\n",
    "    x = tf.keras.layers.Conv2D(filters=output_channels,kernel_size=3,strides=1,padding='same')(inputs)\n",
    "    \n",
    "    #x = last(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67154c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE MODEL\n",
    "# starting at inputs, each line is applied sequentially. \n",
    "# the output of each layer is an image (well, it may have many more than 3 bands)\n",
    "# which will be fed into the next layer.\n",
    "# finally the output will be an image with 4 (number of classes) bands \n",
    "# with the intensity at pixel p in class c reflecting the network's confidence that \n",
    "# that pixel p belongs to class c. \n",
    "# the mask is then created by assigning p to the class which has highest confidence.\n",
    "\n",
    "#best one so far\n",
    "\n",
    "def shallow_model(output_channels:int):\n",
    "    inputs = tf.keras.layers.Input(shape=[None, None, 11])\n",
    "    \n",
    "    #down\n",
    "    x = tf.keras.layers.Conv2D(filters=5,kernel_size=2,strides=2,activation=\"relu\",padding='same')(inputs)\n",
    "    \n",
    "    #maxpool\n",
    "    #x = tf.keras.layers.MaxPool2D(pool_size=(2, 2),padding='same')(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(5,2,2,activation=\"relu\",padding='same')(x)\n",
    "    \n",
    "    #maxpool\n",
    "    #x = tf.keras.layers.MaxPool2D(pool_size=(2, 2),padding='same')(x)\n",
    "    \n",
    "    #up\n",
    "    x = tf.keras.layers.Conv2DTranspose(5,2,2,activation=\"relu\",padding='same')(x)\n",
    "    \n",
    "    last = tf.keras.layers.Conv2DTranspose(\n",
    "    filters=5, kernel_size=2, strides=2,activation=\"relu\",\n",
    "    padding='same')\n",
    "    \n",
    "    x = last(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(output_channels,2,1,activation=\"relu\",padding='same')(x)\n",
    "    #x = tf.keras.layers.MaxPool2D(pool_size=(2, 2),padding='same')(x)\n",
    "\n",
    "    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c83657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPILE THE MODEL\n",
    "\n",
    "#4 classes when tidal flats are unlabeled \n",
    "\n",
    "OUTPUT_CLASSES = 4\n",
    "#OUTPUT_CLASSES=5\n",
    "\n",
    "\n",
    "model = shallow_model(output_channels=OUTPUT_CLASSES)\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'],\n",
    "              weighted_metrics=['accuracy']\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa0872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at architecture\n",
    "\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdd8693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize model\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3dcb26",
   "metadata": {},
   "source": [
    "# Viewing model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74e9112",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VIEWING MODEL OUTPUT\n",
    "# we should probably put legends for the classes, \n",
    "# otherwise its easy to get confused between colors\n",
    "\n",
    "for image,mask in test_dataset.take(2):\n",
    "    sample_image,sample_mask = image, mask\n",
    "for image,mask in train_dataset.take(2):\n",
    "    sample_train_image,sample_train_mask = image, mask\n",
    "\n",
    "def create_mask(pred_mask):\n",
    "  pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "  pred_mask = pred_mask[..., tf.newaxis]\n",
    "  return pred_mask[0]\n",
    "\n",
    "def show_predictions(dataset=None, num=1):\n",
    "  if dataset:\n",
    "    for image, mask in dataset.take(num):\n",
    "      pred_mask = model.predict(image)\n",
    "      display([image, mask[0,0], create_mask(pred_mask)])\n",
    "  else:\n",
    "    display([sample_image, sample_mask,\n",
    "             create_mask(model.predict(sample_image[tf.newaxis, ...]))])\n",
    "\n",
    "def show_train_predictions(dataset=None, num=2):\n",
    "  if dataset:\n",
    "    for image, mask in dataset.take(num):\n",
    "      pred_mask = model.predict(image)\n",
    "      display([image[0], mask[0,0], create_mask(pred_mask)])\n",
    "  else:\n",
    "    display([sample_train_image, sample_train_mask,\n",
    "             create_mask(model.predict(sample_train_image[tf.newaxis, ...]))])\n",
    "    \n",
    "def display(display_list):\n",
    "  plt.figure(figsize=(20, 20))\n",
    "\n",
    "  title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "  for i in range(len(display_list)):\n",
    "    plt.subplot(1,len(display_list),i+1)\n",
    "    plt.title(title[i])\n",
    "    if display_list[i].shape[-1]==1:\n",
    "        mi = 0\n",
    "        ma = 3\n",
    "    else:\n",
    "        mi = 4\n",
    "        ma = 7\n",
    "    plt.imshow(tf.keras.utils.array_to_img(display_list[i][:,:,mi:ma]),vmin=0,vmax=255)\n",
    "    plt.axis('off')\n",
    "  plt.show()\n",
    "# print(pred_mask_int.shape)\n",
    "# print(pred_mask_int[0,90,180,0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b181a0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what our network outputs now\n",
    "# if training hasn't happened should look totally random\n",
    "show_train_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2831a7c",
   "metadata": {},
   "source": [
    "# Saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d26b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING AND LOADING\n",
    "# be very careful to choose the filename so as not to overwrite a good model that \n",
    "# took hours or days to train!\n",
    "\n",
    "#saving\n",
    "model.save('saved-models/VCR_AGU23')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af371a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading\n",
    "model = tf.keras.models.load_model('saved-models/Bmodel-5000Oct24')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053db10c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfa4245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING MODEL\n",
    "\n",
    "#callback is just to do display info during training\n",
    "class DisplayCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    if epoch % 5==0:\n",
    "        clear_output(wait=True)\n",
    "        #show_predictions()\n",
    "        show_train_predictions()\n",
    "    print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))\n",
    "    \n",
    "# be careful to send your logs to a useful filename, right now it's /logs/Demo\n",
    "\n",
    "tb_callback = tf.keras.callbacks.TensorBoard('./logs/test', update_freq=5)\n",
    "\n",
    "## adding sample weights so that the final class is ignored \n",
    "\n",
    "def add_sample_weights(image, label):\n",
    "  # The weights for each class, with the constraint that:\n",
    "  #     sum(class_weights) == 1.0\n",
    "  class_weights = tf.constant([1.5/counts[0],1.5/counts[1],1/counts[2], 0.0])\n",
    "  #class_weights = tf.constant(inv_freq)\n",
    "  class_weights = class_weights/tf.reduce_sum(tf.abs(class_weights))\n",
    "\n",
    "  # Create an image of `sample_weights` by using the label at each pixel as an \n",
    "  # index into the `class weights` .\n",
    "  sample_weights = tf.gather(class_weights, indices=tf.cast(label, tf.int32))\n",
    "\n",
    "  return image, label, sample_weights\n",
    "\n",
    "#actually training \n",
    "\n",
    "EPOCHS = 1000\n",
    "# 10 epochs for demo \n",
    "#change to 3000 or so to retrain for real\n",
    "\n",
    "model_history = model.fit(train_batches.map(add_sample_weights), \n",
    "                          #class_weight = {0:1,1:1,2:1,3:1,4:0},\n",
    "                          epochs=EPOCHS,\n",
    "                          #steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                          #validation_steps=VALIDATION_STEPS,\n",
    "                          validation_data=test_batches.map(add_sample_weights),\n",
    "                          callbacks=[DisplayCallback(),tb_callback]\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458d451b",
   "metadata": {},
   "source": [
    "# logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2879658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to reload_ext\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6007106",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4d3a54",
   "metadata": {},
   "source": [
    "# Analyzing the predictions (confusion matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0ae9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "COLOR='red'\n",
    "plt.rcParams['text.color'] = COLOR\n",
    "plt.rcParams['axes.labelcolor'] = COLOR\n",
    "plt.rcParams['xtick.color'] = COLOR\n",
    "plt.rcParams['ytick.color'] = COLOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c75583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_masks_int = tf.cast(test_masks, dtype=tf.int32)\n",
    "class_list = list(class_dict.values())\n",
    "class_list.remove('unlabeled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56470d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_full(pred_mask):\n",
    "     pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "     pred_mask = pred_mask[..., tf.newaxis]\n",
    "     return pred_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1a344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mask = model.predict(test_images)\n",
    "pred_mask.shape\n",
    "pred_mask_int=create_mask_full(pred_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47871482",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mtx = tf.math.confusion_matrix(tf.reshape(test_masks_int, [-1]), tf.reshape(pred_mask_int, [-1]))[:3,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bd320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sums = np.sum(confusion_mtx, axis=1)\n",
    "col_sums = np.sum(confusion_mtx, axis=0)\n",
    "confusion_mtx_user = np.diag(1/row_sums) @ confusion_mtx.numpy()\n",
    "confusion_mtx_pdsr = confusion_mtx.numpy() @ np.diag(1/col_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820e75fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unnormalized\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_mtx, xticklabels=class_list, yticklabels=class_list, \n",
    "            annot=True, fmt='g')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952b9314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalized truth\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_mtx_pdsr, xticklabels=class_list, yticklabels=class_list, \n",
    "            annot=True, fmt='g')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fab2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
